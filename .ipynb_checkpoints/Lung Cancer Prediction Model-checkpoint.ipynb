{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2772d0a",
   "metadata": {},
   "source": [
    "# Data science project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f24bc2",
   "metadata": {},
   "source": [
    "Saadan teile failid .zip kujul. CSV failides on kaks andmetabelit, mis koosnevad sÃ¼nteetilitest andmetest, mis pÃµhinevad reaalsete vÃ¤hihaigete (kopsu ja eesnÃ¤Ã¤rme) haigustrajektooridel. Andmetabelitel on kolm tunnust SUBJECT_ID - unikaalne patsiendi id; DEFINITION_ID - meditsiiniline sekkumine, mis patsiendiga toimus; TIME - aeg aastates, millal sekkumine toimus. Ajal 0 sai iga patsient vÃ¤hidiagnoosi. Osadel patsientidest on olemas seisund \"death\", mis tÃ¤hendab, et see patsient suri antud ajahetkel. KÃµikide patsientide puhul on kustutatud aasta enne suremist/viimast Ã¼leskirjutist.\n",
    "\n",
    "Teie Ã¼lesanne on ennustada patsientide suremust 1-aasta kÃ¤igus (mil andmed lÃµppevad), ehk siis klassifitseerimisÃ¼lesanne, kus klass 1 - patsient sureb; 0 - patsient jÃ¤Ã¤b aasta jooksul ellu. Ãœks viis andmeid tÃ¶Ã¶delda, selleks, et neid mudelitele ette sÃ¶Ã¶ta, on iga unikaalne sekkumine ja selle jÃ¤rjekorra number muuta eraldi tunnuseks, tekitada iga patsiendi kohta Ã¼ks rida ja vÃ¤Ã¤rtuseks vÃµtta aeg. Kuid igasugune andmete tÃ¶Ã¶tlemine on teie enda vaba valik. Te vÃµite tekitada klassifitseerija Ã¼kskÃµik millisel meetodil: otsustuspuud, nÃ¤rvivÃµrgud, logistiline regressioon vms. \n",
    "\n",
    "Ãœks peamine probleem terviseandmetega on tÃµsisasi, et need on Ã¼sna mÃ¼rased. Erinevate sekkumiste all on peidetud definitsioonid, millest osad on arvatavasti ennustamiseks olulised (nÃ¤iteks \"keemiaravi\") ja teised mitte nii vÃ¤ga (nÃ¤iteks \"nÃ¤gemiskontroll\"). MÃ¼raste andmete/tunnuste eemaldamine vÃµib antud Ã¼lesande puhul suuresti tÃµsta mudeli tÃ¤psust. KÃ¤esoleva Ã¼lesande juures on tÃ¤htis, et te ei vÃµi eeldada, et sekkumiste definitsioonid on erinevate andmetabelite seas sama tÃ¤hendusega (antud juhul nÃ¤iteks, et Drug_1 on sama, mis Drug_1 teises andmestikus).\n",
    "\n",
    "TÃ¶Ã¶ tulemuseks peaks olema tÃ¶Ã¶voog, mis vÃµtab samasuguse andmetabeli sisendiks ning vÃ¤ljastaks AUC-ROC vÃ¤Ã¤rtuse ja mudeli enda. Seda vÃµib teha nii jupyter-notebookis kui ka skripti kujul.\n",
    "\n",
    "ZIP failist leiate kaks andmetabelit, vÃµite Ã¼ht kasutada tÃ¶Ã¶voo loomiseks ja teist selleks, et seda valideerida. Andmed on sÃ¼nteetilised ja pole Ã¼ksteisega seotud (SUBJECT_ID vÃ¤Ã¤rtus 1 Ã¼hes ei tÃ¤henda, et tegu on sama inimesega teises andmetabelis). Mul endal on ligipÃ¤Ã¤s pÃ¤ris andmetele, mida ma teiega kahjuks jagada ei saa, aga ma vÃµin jooksutada teie tÃ¶Ã¶voogu pÃ¤ris andmetel ja jagada teiega tulemusi. Te vÃµite vÃ¤ljundisse lisada erinevat statistikat, mida soovite pÃ¤risandmetelt saada. Need vÃ¤ljundid peavad olema summeeritud, mis tÃ¤hendab, et ma ei saa teiega jagada patsiendi tasemel andmeid.\n",
    "\n",
    "Ma ise loodan, et teie loodud tÃ¶Ã¶voog selekteeriks tunnuseid. Parim terviseandmete mudel on vÃµimalikult vÃ¤ikese arvu tunnustega, samuti teades neid tunnuseid saab edendada teadustÃ¶Ã¶d nii haiguste kui ka nende Ã¤rahoidmise/ravimise kohta. Samuti soovitan julgelt proovida graafi loomist ning sellelt \"key-playerite\" jms leidmist, see oleks innovaatiline viis tunnuste valimiseks.\n",
    "\n",
    "FYI, neid samu andmeid kasutades sain ma juhumetsaga AUCROC'i 0.65 ja pÃ¤rast tunnuste selekteerimist oli AUCROC 0.75. Juhul kui tÃ¶Ã¶ tulemus on hea ja kasutatav ka muude haiguste ennustusmudelite loomisel on vÃµimalus saada oma nimi teadusartiklile ðŸ™‚ \n",
    "\n",
    " Hello!\n",
    "\n",
    "I was contacted regarding the concern about death times occurring later than a year from last medical intervention. The problem is present because of my coding error ðŸ™‚ This is okay and does not mess up the overall purpose of your workflow. You can consider all persons who have the \"death\"  state as dying within one year.\n",
    "\n",
    "Sorry about the confusion,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e90af1",
   "metadata": {},
   "source": [
    "# Lung Cancer Survival Prediction with Synthetic Data\n",
    "\n",
    "Objective:\n",
    "Utilize synthetic medical data to predict 5-year survival rates post-lung cancer diagnosis by effectively selecting key features from patient treatment trajectories.\n",
    "Possible approach:\n",
    "1. Feature Selection: Use algorithms like PCA and Stepwise Selection to identify crucial features.\n",
    "2. Subsequence Analysis: Find frequent subsequences within treatment trajectories for additional predictive value. Effectiveness (performance) of the algorithm is important.\n",
    "3. Use network science approach. Using graphs and extracting key-player nodes as features and using them for prediction.\n",
    "4. Any custom approach.\n",
    "End result:\n",
    "Predictive Model: Develop a workflow that, given patient treatment trajectory data, outputs a 5-year survival prediction. Workflow will be tested on a test set not seen by students and compared with other groups work.\n",
    "Outcome:\n",
    "A concise list of significant features, an understanding of vital treatment subsequences, and a prediction function for stakeholder use. Possibility to get published!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5090b735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(        SUBJECT_ID     DEFINITION_ID      TIME\n",
       " 0                1          drug_217  0.004807\n",
       " 1                1    condition_1922  0.008643\n",
       " 2                1     condition_785  0.027792\n",
       " 3                1           drug_49  0.032515\n",
       " 4                1   measurement_132  0.056765\n",
       " ...            ...               ...       ...\n",
       " 560966         984  measurement_1141  0.027321\n",
       " 560967         984   observation_156  0.028739\n",
       " 560968         984  measurement_1140  0.030802\n",
       " 560969         984  measurement_1327  0.035081\n",
       " 560970         984     condition_459  0.038022\n",
       " \n",
       " [560971 rows x 3 columns],\n",
       "         SUBJECT_ID     DEFINITION_ID      TIME\n",
       " 0                1     condition_141  0.019808\n",
       " 1                1     condition_144  0.068319\n",
       " 2                1          drug_108  0.069314\n",
       " 3                1     condition_621  0.093951\n",
       " 4                1          drug_283  0.133907\n",
       " ...            ...               ...       ...\n",
       " 542732         777   measurement_441  4.168139\n",
       " 542733         777   measurement_153  4.170336\n",
       " 542734         777  measurement_1245  4.632134\n",
       " 542735         777   measurement_831  4.637782\n",
       " 542736         777     condition_684  4.650676\n",
       " \n",
       " [542737 rows x 3 columns])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the files\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "data=pd.read_csv(\"synthetic_data_lung_cancer.csv\")\n",
    "validation=pd.read_csv(\"synthetic_data_pca.csv\")\n",
    "\n",
    "data,validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eb2be3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features_and_labels(df):\n",
    "    \"\"\"\n",
    "    Prepares the features and labels for model training from the dataset.\n",
    "    \n",
    "    Args:\n",
    "    - df: A pandas DataFrame containing the data with 'SUBJECT_ID', 'DEFINITION_ID', and 'TIME' columns.\n",
    "    \n",
    "    Returns:\n",
    "    - X: Features DataFrame ready for training.\n",
    "    - y: Labels Series indicating whether death occurred within a year.\n",
    "    \"\"\"\n",
    "    # Step 1: Extract general categories from DEFINITION_ID\n",
    "    df['category'] = df['DEFINITION_ID'].str.extract(r'(\\D+)')\n",
    "\n",
    "    # Step 2: Count occurrences of each category per patient\n",
    "    category_counts = df.pivot_table(index='SUBJECT_ID', columns='category', aggfunc='size', fill_value=0)\n",
    "\n",
    "    # Step 3: Calculate time-based features\n",
    "    df['time_of_first_intervention'] = df.groupby('SUBJECT_ID')['TIME'].transform('min')\n",
    "    df['time_since_last_intervention'] = df.groupby('SUBJECT_ID')['TIME'].transform('max')\n",
    "    df['duration_of_treatment'] = df['time_since_last_intervention'] - df['time_since_first_intervention']\n",
    "    df['average_time_between_interventions'] = df.groupby('SUBJECT_ID')['TIME'].transform(lambda x: x.diff().mean() if len(x) > 1 else 0)\n",
    "\n",
    "    # Aggregate these features into a single dataframe, ensuring one row per patient\n",
    "    time_features = df.groupby('SUBJECT_ID')[['time_since_first_intervention', 'time_since_last_intervention', \n",
    "                                              'duration_of_treatment', 'average_time_between_interventions']].first().reset_index()\n",
    "\n",
    "    # Combine counts of categories with time-based features\n",
    "    features_data = pd.merge(category_counts, time_features, on='SUBJECT_ID')\n",
    "\n",
    "    # Step 4: Identify 'death' events and create the binary target variable\n",
    "    death_events = df[df['DEFINITION_ID'] == 'death']['SUBJECT_ID'].unique()\n",
    "    features_data['death'] = features_data['SUBJECT_ID'].isin(death_events).astype(int)\n",
    "\n",
    "    # Drop 'SUBJECT_ID' as it is not a feature\n",
    "    features_data.drop('SUBJECT_ID', axis=1, inplace=True)\n",
    "\n",
    "    # Split the combined data into features and labels\n",
    "    X = features_data.drop('death', axis=1)\n",
    "    y = features_data['death']\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Example usage:\n",
    "# X_train, y_train = prepare_features_and_labels(training_data)\n",
    "# X_validation, y_validation = prepare_features_and_labels(validation_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748edd3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73090254",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>condition_</th>\n",
       "      <th>drug_</th>\n",
       "      <th>measurement_</th>\n",
       "      <th>observation_</th>\n",
       "      <th>procedure_</th>\n",
       "      <th>time_since_first_intervention</th>\n",
       "      <th>time_since_last_intervention</th>\n",
       "      <th>duration_of_treatment</th>\n",
       "      <th>average_time_between_interventions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>261</td>\n",
       "      <td>195</td>\n",
       "      <td>33</td>\n",
       "      <td>48</td>\n",
       "      <td>38</td>\n",
       "      <td>0.004807</td>\n",
       "      <td>4.932455</td>\n",
       "      <td>4.927648</td>\n",
       "      <td>0.008585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>95</td>\n",
       "      <td>45</td>\n",
       "      <td>88</td>\n",
       "      <td>78</td>\n",
       "      <td>57</td>\n",
       "      <td>0.003203</td>\n",
       "      <td>2.518113</td>\n",
       "      <td>2.514911</td>\n",
       "      <td>0.006928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>144</td>\n",
       "      <td>59</td>\n",
       "      <td>561</td>\n",
       "      <td>69</td>\n",
       "      <td>95</td>\n",
       "      <td>0.001870</td>\n",
       "      <td>3.994337</td>\n",
       "      <td>3.992467</td>\n",
       "      <td>0.004307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "      <td>11</td>\n",
       "      <td>385</td>\n",
       "      <td>22</td>\n",
       "      <td>50</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>4.002734</td>\n",
       "      <td>4.002636</td>\n",
       "      <td>0.008037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30</td>\n",
       "      <td>8</td>\n",
       "      <td>131</td>\n",
       "      <td>34</td>\n",
       "      <td>26</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>5.894177</td>\n",
       "      <td>5.894027</td>\n",
       "      <td>0.025851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>722</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>0.018921</td>\n",
       "      <td>0.288566</td>\n",
       "      <td>0.269644</td>\n",
       "      <td>0.005503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>723</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>14</td>\n",
       "      <td>29</td>\n",
       "      <td>0.006720</td>\n",
       "      <td>0.273975</td>\n",
       "      <td>0.267254</td>\n",
       "      <td>0.002304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>0.094755</td>\n",
       "      <td>0.094565</td>\n",
       "      <td>0.001478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>0.001208</td>\n",
       "      <td>3.149692</td>\n",
       "      <td>3.148484</td>\n",
       "      <td>0.174916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000704</td>\n",
       "      <td>0.038022</td>\n",
       "      <td>0.037318</td>\n",
       "      <td>0.000602</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>727 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     condition_  drug_  measurement_  observation_  procedure_  \\\n",
       "0           261    195            33            48          38   \n",
       "1            95     45            88            78          57   \n",
       "2           144     59           561            69          95   \n",
       "3            30     11           385            22          50   \n",
       "4            30      8           131            34          26   \n",
       "..          ...    ...           ...           ...         ...   \n",
       "722          13      0            22             6           9   \n",
       "723          11      0            63            14          29   \n",
       "724           2      0            62             1           0   \n",
       "725           4      0             1             6           7   \n",
       "726           5      0            46             6           6   \n",
       "\n",
       "     time_since_first_intervention  time_since_last_intervention  \\\n",
       "0                         0.004807                      4.932455   \n",
       "1                         0.003203                      2.518113   \n",
       "2                         0.001870                      3.994337   \n",
       "3                         0.000098                      4.002734   \n",
       "4                         0.000150                      5.894177   \n",
       "..                             ...                           ...   \n",
       "722                       0.018921                      0.288566   \n",
       "723                       0.006720                      0.273975   \n",
       "724                       0.000190                      0.094755   \n",
       "725                       0.001208                      3.149692   \n",
       "726                       0.000704                      0.038022   \n",
       "\n",
       "     duration_of_treatment  average_time_between_interventions  \n",
       "0                 4.927648                            0.008585  \n",
       "1                 2.514911                            0.006928  \n",
       "2                 3.992467                            0.004307  \n",
       "3                 4.002636                            0.008037  \n",
       "4                 5.894027                            0.025851  \n",
       "..                     ...                                 ...  \n",
       "722               0.269644                            0.005503  \n",
       "723               0.267254                            0.002304  \n",
       "724               0.094565                            0.001478  \n",
       "725               3.148484                            0.174916  \n",
       "726               0.037318                            0.000602  \n",
       "\n",
       "[727 rows x 9 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "76609a5d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'time_since_first_intervention'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'time_since_first_intervention'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X, y \u001b[38;5;241m=\u001b[39m prepare_features_and_labels(data)\n\u001b[0;32m      2\u001b[0m X_validation, y_validation \u001b[38;5;241m=\u001b[39m prepare_features_and_labels(validation)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Define the parameter grid to search\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[37], line 21\u001b[0m, in \u001b[0;36mprepare_features_and_labels\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     19\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_of_first_intervention\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSUBJECT_ID\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTIME\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     20\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_since_last_intervention\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSUBJECT_ID\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTIME\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 21\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mduration_of_treatment\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_since_last_intervention\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m-\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_since_first_intervention\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     22\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maverage_time_between_interventions\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSUBJECT_ID\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTIME\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mdiff()\u001b[38;5;241m.\u001b[39mmean() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Aggregate these features into a single dataframe, ensuring one row per patient\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'time_since_first_intervention'"
     ]
    }
   ],
   "source": [
    "X, y = prepare_features_and_labels(data)\n",
    "X_validation, y_validation = prepare_features_and_labels(validation)\n",
    "\n",
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],  # Number of trees in the forest\n",
    "    'max_depth': [None, 10, 20, 30],  # Maximum depth of the tree\n",
    "    'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2, 4],    # Minimum number of samples required to be at a leaf node\n",
    "    # Add other parameters here if you want\n",
    "}\n",
    "\n",
    "# Initialize the RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV with the classifier, parameter grid, and desired scoring metric\n",
    "grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, \n",
    "                           cv=5, n_jobs=-1, scoring='roc_auc', verbose=2)\n",
    "\n",
    "# Fit GridSearchCV to the training data\n",
    "grid_search.fit(X, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b634b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best estimator\n",
    "best_rf_classifier = grid_search.best_estimator_\n",
    "\n",
    "# Optionally, evaluate on the validation set\n",
    "X_validation = X_validation.reindex(columns=X.columns, fill_value=0)  # Ensuring column match\n",
    "y_pred_proba_validation = best_rf_classifier.predict_proba(X_validation)[:, 1]\n",
    "auc_roc_validation = roc_auc_score(y_validation, y_pred_proba_validation)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best AUC-ROC on Validation:\", auc_roc_validation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846d980c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a subset of the data to create a smaller graph for analysis\n",
    "\n",
    "# We will take a random sample of patients and their corresponding interventions\n",
    "\n",
    "\n",
    "# Since the data may be ordered by SUBJECT_ID, we shuffle the data to get a random sample\n",
    "\n",
    "data_sample = features.sample(frac=1, random_state=1)\n",
    "\n",
    "data_sample.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "# Let's take a sample of 5% of the data to make the graph computation tractable\n",
    "\n",
    "sample_size = int(0.5 * len(data_sample))\n",
    "\n",
    "data_sample = data_sample.head(sample_size)\n",
    "\n",
    "\n",
    "\n",
    "# Create a new graph from the sampled data\n",
    "\n",
    "G_sample = nx.Graph()\n",
    "\n",
    "\n",
    "\n",
    "# Add nodes and edges from the sampled dataset\n",
    "\n",
    "for index, row in data_sample.iterrows():\n",
    "\n",
    "    # Add node if it doesn't exist\n",
    "\n",
    "    if not G_sample.has_node(row['DEFINITION_ID']):\n",
    "\n",
    "        G_sample.add_node(row['DEFINITION_ID'])\n",
    "\n",
    "    \n",
    "\n",
    "    # Since the data is ordered by time for each SUBJECT_ID, we can link sequential interventions\n",
    "\n",
    "    if index > 0 and data_sample.iloc[index - 1]['SUBJECT_ID'] == row['SUBJECT_ID']:\n",
    "\n",
    "        prev_definition_id = data_sample.iloc[index - 1]['DEFINITION_ID']\n",
    "\n",
    "        # If edge does not exist, add with weight 1, otherwise increase weight\n",
    "\n",
    "        if not G_sample.has_edge(prev_definition_id, row['DEFINITION_ID']):\n",
    "\n",
    "            G_sample.add_edge(prev_definition_id, row['DEFINITION_ID'], weight=1)\n",
    "\n",
    "        else:\n",
    "\n",
    "            G_sample[prev_definition_id][row['DEFINITION_ID']]['weight'] += 1\n",
    "\n",
    "\n",
    "# Calculate centrality measures on the sample graph again\n",
    "\n",
    "degree_centrality_sample = nx.degree_centrality(G_sample)\n",
    "\n",
    "betweenness_centrality_sample = nx.betweenness_centrality(G_sample)\n",
    "\n",
    "closeness_centrality_sample = nx.closeness_centrality(G_sample)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d398801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify key players using degree centrality from the full graph\n",
    "key_players_by_degree_full = sorted(degree_centrality_sample.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "# Retrieve the top 100 key players from the full dataset\n",
    "top_key_players_full = key_players_by_degree_full[:300]\n",
    "\n",
    "top_key_players_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90868493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will now split the data with the corrected labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    key_features_data_scaled, pivot_data_label, test_size=0.2, random_state=42, stratify=pivot_data_label)\n",
    "\n",
    "# Train the logistic regression model using the corrected data\n",
    "logreg = LogisticRegression(max_iter=10000)\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities for the test set\n",
    "y_pred_proba = logreg.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate the AUC-ROC score for the test set\n",
    "auc_roc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "auc_roc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdb1c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Apply SMOTE to the training data only\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Initialize the Random Forest Classifier\n",
    "rf_smote = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Train the Random Forest Classifier on the SMOTE-resampled training data\n",
    "rf_smote.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Predict probabilities for the test set\n",
    "y_pred_proba_rf_smote = rf_smote.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate the AUC-ROC score for the test set using the Random Forest model trained on SMOTE data\n",
    "auc_roc_rf_smote = roc_auc_score(y_test, y_pred_proba_rf_smote)\n",
    "\n",
    "# Output the SMOTE class distribution and the AUC-ROC score\n",
    "smote_class_distribution = pd.Series(y_train_smote).value_counts(), auc_roc_rf_smote\n",
    "\n",
    "auc_roc_rf_smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d8371b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#old code\n",
    "\n",
    "# Correcting the approach to create the target label 'death'\n",
    "\n",
    "# Recreate the pivot table for the development dataset\n",
    "pivot_data = data.pivot_table(index='SUBJECT_ID', columns='DEFINITION_ID', values='TIME', aggfunc='first')\n",
    "\n",
    "# Correctly identify the 'death' entries within the 'DEFINITION_ID' column and assign the binary label\n",
    "# '1' for death within a year and '0' for survival\n",
    "death_events = data[data['DEFINITION_ID'] == 'death']['SUBJECT_ID'].unique()\n",
    "pivot_data['death'] = pivot_data.index.isin(death_events).astype(int)\n",
    "\n",
    "# Drop the 'death' column from features\n",
    "pivot_data_features = pivot_data.drop('death', axis=1)\n",
    "pivot_data_label = pivot_data['death']\n",
    "\n",
    "# Display the first few rows of the features and the distribution of the label\n",
    "pivot_data_features_head = pivot_data_features.head()\n",
    "pivot_data_label_distribution = pivot_data_label.value_counts()\n",
    "\n",
    "pivot_data_features_head, pivot_data_label_distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa06f71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
