{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2772d0a",
   "metadata": {},
   "source": [
    "# Data science project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f24bc2",
   "metadata": {},
   "source": [
    "Saadan teile failid .zip kujul. CSV failides on kaks andmetabelit, mis koosnevad sÃ¼nteetilitest andmetest, mis pÃµhinevad reaalsete vÃ¤hihaigete (kopsu ja eesnÃ¤Ã¤rme) haigustrajektooridel. Andmetabelitel on kolm tunnust SUBJECT_ID - unikaalne patsiendi id; DEFINITION_ID - meditsiiniline sekkumine, mis patsiendiga toimus; TIME - aeg aastates, millal sekkumine toimus. Ajal 0 sai iga patsient vÃ¤hidiagnoosi. Osadel patsientidest on olemas seisund \"death\", mis tÃ¤hendab, et see patsient suri antud ajahetkel. KÃµikide patsientide puhul on kustutatud aasta enne suremist/viimast Ã¼leskirjutist.\n",
    "\n",
    "Teie Ã¼lesanne on ennustada patsientide suremust 1-aasta kÃ¤igus (mil andmed lÃµppevad), ehk siis klassifitseerimisÃ¼lesanne, kus klass 1 - patsient sureb; 0 - patsient jÃ¤Ã¤b aasta jooksul ellu. Ãœks viis andmeid tÃ¶Ã¶delda, selleks, et neid mudelitele ette sÃ¶Ã¶ta, on iga unikaalne sekkumine ja selle jÃ¤rjekorra number muuta eraldi tunnuseks, tekitada iga patsiendi kohta Ã¼ks rida ja vÃ¤Ã¤rtuseks vÃµtta aeg. Kuid igasugune andmete tÃ¶Ã¶tlemine on teie enda vaba valik. Te vÃµite tekitada klassifitseerija Ã¼kskÃµik millisel meetodil: otsustuspuud, nÃ¤rvivÃµrgud, logistiline regressioon vms. \n",
    "\n",
    "Ãœks peamine probleem terviseandmetega on tÃµsisasi, et need on Ã¼sna mÃ¼rased. Erinevate sekkumiste all on peidetud definitsioonid, millest osad on arvatavasti ennustamiseks olulised (nÃ¤iteks \"keemiaravi\") ja teised mitte nii vÃ¤ga (nÃ¤iteks \"nÃ¤gemiskontroll\"). MÃ¼raste andmete/tunnuste eemaldamine vÃµib antud Ã¼lesande puhul suuresti tÃµsta mudeli tÃ¤psust. KÃ¤esoleva Ã¼lesande juures on tÃ¤htis, et te ei vÃµi eeldada, et sekkumiste definitsioonid on erinevate andmetabelite seas sama tÃ¤hendusega (antud juhul nÃ¤iteks, et Drug_1 on sama, mis Drug_1 teises andmestikus).\n",
    "\n",
    "TÃ¶Ã¶ tulemuseks peaks olema tÃ¶Ã¶voog, mis vÃµtab samasuguse andmetabeli sisendiks ning vÃ¤ljastaks AUC-ROC vÃ¤Ã¤rtuse ja mudeli enda. Seda vÃµib teha nii jupyter-notebookis kui ka skripti kujul.\n",
    "\n",
    "ZIP failist leiate kaks andmetabelit, vÃµite Ã¼ht kasutada tÃ¶Ã¶voo loomiseks ja teist selleks, et seda valideerida. Andmed on sÃ¼nteetilised ja pole Ã¼ksteisega seotud (SUBJECT_ID vÃ¤Ã¤rtus 1 Ã¼hes ei tÃ¤henda, et tegu on sama inimesega teises andmetabelis). Mul endal on ligipÃ¤Ã¤s pÃ¤ris andmetele, mida ma teiega kahjuks jagada ei saa, aga ma vÃµin jooksutada teie tÃ¶Ã¶voogu pÃ¤ris andmetel ja jagada teiega tulemusi. Te vÃµite vÃ¤ljundisse lisada erinevat statistikat, mida soovite pÃ¤risandmetelt saada. Need vÃ¤ljundid peavad olema summeeritud, mis tÃ¤hendab, et ma ei saa teiega jagada patsiendi tasemel andmeid.\n",
    "\n",
    "Ma ise loodan, et teie loodud tÃ¶Ã¶voog selekteeriks tunnuseid. Parim terviseandmete mudel on vÃµimalikult vÃ¤ikese arvu tunnustega, samuti teades neid tunnuseid saab edendada teadustÃ¶Ã¶d nii haiguste kui ka nende Ã¤rahoidmise/ravimise kohta. Samuti soovitan julgelt proovida graafi loomist ning sellelt \"key-playerite\" jms leidmist, see oleks innovaatiline viis tunnuste valimiseks.\n",
    "\n",
    "FYI, neid samu andmeid kasutades sain ma juhumetsaga AUCROC'i 0.65 ja pÃ¤rast tunnuste selekteerimist oli AUCROC 0.75. Juhul kui tÃ¶Ã¶ tulemus on hea ja kasutatav ka muude haiguste ennustusmudelite loomisel on vÃµimalus saada oma nimi teadusartiklile ðŸ™‚ \n",
    "\n",
    " Hello!\n",
    "\n",
    "I was contacted regarding the concern about death times occurring later than a year from last medical intervention. The problem is present because of my coding error ðŸ™‚ This is okay and does not mess up the overall purpose of your workflow. You can consider all persons who have the \"death\"  state as dying within one year.\n",
    "\n",
    "Sorry about the confusion,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e90af1",
   "metadata": {},
   "source": [
    "# Lung Cancer Survival Prediction with Synthetic Data\n",
    "\n",
    "Objective:\n",
    "Utilize synthetic medical data to predict 5-year survival rates post-lung cancer diagnosis by effectively selecting key features from patient treatment trajectories.\n",
    "Possible approach:\n",
    "1. Feature Selection: Use algorithms like PCA and Stepwise Selection to identify crucial features.\n",
    "2. Subsequence Analysis: Find frequent subsequences within treatment trajectories for additional predictive value. Effectiveness (performance) of the algorithm is important.\n",
    "3. Use network science approach. Using graphs and extracting key-player nodes as features and using them for prediction.\n",
    "4. Any custom approach.\n",
    "End result:\n",
    "Predictive Model: Develop a workflow that, given patient treatment trajectory data, outputs a 5-year survival prediction. Workflow will be tested on a test set not seen by students and compared with other groups work.\n",
    "Outcome:\n",
    "A concise list of significant features, an understanding of vital treatment subsequences, and a prediction function for stakeholder use. Possibility to get published!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2f1c1a",
   "metadata": {},
   "source": [
    "\n",
    "# Workflow 1 - modifying the data by creating new features. \n",
    "\n",
    "The method roc_rf_new_features(df) takes in the dataframe name and trains a random forest classifier on it. The workflow modifies the dataframe to create new features that the model is trained on. The method also outputs a ROC graph and a feature importance graph and returns the random forest classifier.\n",
    "\n",
    "The method rf_new_features_survival_prediction(df,rf) takes in a dataframe and a random forest classifier and returns the prediction for each subject. Values close to 0 imply survival, 1 implies death.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbea5fde",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m data_sample\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Create the graph from the sampled data again\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m G_sample \u001b[38;5;241m=\u001b[39m nx\u001b[38;5;241m.\u001b[39mGraph()\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Add nodes and edges from the sampled dataset\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m data_sample\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m# Add node if it doesn't exist\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nx' is not defined"
     ]
    }
   ],
   "source": [
    "#rf_wf1=roc_rf_new_features(\"synthetic_data_Lung_cancer.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "958cb52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rf_new_features_survival_prediction(\"synthetic_data_Lung_cancer.csv\",rf_wf1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54658fec",
   "metadata": {},
   "source": [
    "# Workflow 2 - feature selection by using a graph\n",
    "\n",
    "The method roc_rf_with_feature_selection_graph(df) takes in the name of a dataframe and trains a random forest classifier on it. The workflow creates a graph from a 20% sample of the data to find the top 100 key values in 'DEFINITION_id'. The data is then modified to have those those values as features and a model is trained on it. The method outputs the ROC AUC score, a ROC graph and the 100 key features and their corresponding weights.\n",
    "\n",
    "### Make sure to run the other cells then uncomment to run the methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2c8c829",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#rf_wf2=roc_rf_with_feature_selection_graph(\"synthetic_data_Lung_cancer.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5090b735",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51637f85",
   "metadata": {},
   "source": [
    "# Code for workflow 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb2be3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features_and_labels(filename):\n",
    "    \n",
    "    df=pd.read_csv(filename)\n",
    "\n",
    "    # Extract general categories and append '_count' only if DEFINITION_ID contains '_'\n",
    "    df['category'] = df.apply(lambda row: row['DEFINITION_ID'].split('_')[0] + '_count' if '_' in row['DEFINITION_ID'] else row['DEFINITION_ID'].split('_')[0], axis=1)\n",
    "\n",
    "    # Step 2: Count occurrences of each category per patient\n",
    "    category_counts = df.pivot_table(index='SUBJECT_ID', columns='category', aggfunc='size', fill_value=0)\n",
    "\n",
    "    # Step 3: Calculate time-based features\n",
    "    df['time_of_first_intervention'] = df.groupby('SUBJECT_ID')['TIME'].transform('min')\n",
    "    df['time_since_last_intervention'] = df.groupby('SUBJECT_ID')['TIME'].transform('max')\n",
    "    df['duration_of_treatment'] = df['time_since_last_intervention'] - df['time_of_first_intervention']\n",
    "    df['average_time_between_interventions'] = df.groupby('SUBJECT_ID')['TIME'].transform(lambda x: x.diff().mean() if len(x) > 1 else 0)\n",
    "\n",
    "    # Aggregate these features into a single dataframe, ensuring one row per patient\n",
    "    time_features = df.groupby('SUBJECT_ID')[['time_of_first_intervention', 'time_since_last_intervention', \n",
    "                                              'duration_of_treatment', 'average_time_between_interventions']].first().reset_index()\n",
    "\n",
    "    # Combine counts of categories with time-based features\n",
    "    features_data = pd.merge(category_counts, time_features, on='SUBJECT_ID')\n",
    "\n",
    "    # Step 4: Identify 'death' events and create the binary target variable\n",
    "    death_events = df[df['DEFINITION_ID'] == 'death']['SUBJECT_ID'].unique()\n",
    "    features_data['death'] = features_data['SUBJECT_ID'].isin(death_events).astype(int)\n",
    "\n",
    "    # Drop 'SUBJECT_ID' as it is not a feature\n",
    "    features_data.drop('SUBJECT_ID', axis=1, inplace=True)\n",
    "\n",
    "    # Split the combined data into features and labels\n",
    "    X = features_data.drop('death', axis=1)\n",
    "    y = features_data['death']\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def rf_new_features_survival_prediction(data, rf_classifier):\n",
    "    \n",
    "    df=pd.read_csv(data)\n",
    "    \n",
    "    df = df[(df['DEFINITION_ID'] != 'death')]\n",
    "    # Extract general categories from DEFINITION_ID and append '_count'\n",
    "    df['category'] = df.apply(lambda row: row['DEFINITION_ID'].split('_')[0] + '_count' if '_' in row['DEFINITION_ID'] else row['DEFINITION_ID'].split('_')[0], axis=1)\n",
    "    # Step 2: Count occurrences of each category per patient\n",
    "    category_counts = df.pivot_table(index='SUBJECT_ID', columns='category', aggfunc='size', fill_value=0)\n",
    "     # Step 3: Calculate time-based features\n",
    "    df['time_of_first_intervention'] = df.groupby('SUBJECT_ID')['TIME'].transform('min')\n",
    "    df['time_since_last_intervention'] = df.groupby('SUBJECT_ID')['TIME'].transform('max')\n",
    "    df['duration_of_treatment'] = df['time_since_last_intervention'] - df['time_of_first_intervention']\n",
    "    df['average_time_between_interventions'] = df.groupby('SUBJECT_ID')['TIME'].transform(lambda x: x.diff().mean() if len(x) > 1 else 0)\n",
    "\n",
    "    # Aggregate these features into a single dataframe, ensuring one row per patient\n",
    "    time_features = df.groupby('SUBJECT_ID')[['time_of_first_intervention', 'time_since_last_intervention', \n",
    "                                              'duration_of_treatment', 'average_time_between_interventions']].first().reset_index()\n",
    "        # Combine counts of categories with time-based features\n",
    "    features_data = pd.merge(category_counts, time_features, on='SUBJECT_ID')\n",
    "    \n",
    "    # Keep 'SUBJECT_ID' for indexing\n",
    "    subject_ids = features_data['SUBJECT_ID']\n",
    "\n",
    "    # Drop 'SUBJECT_ID' as it is not a feature for prediction\n",
    "    features_data.drop('SUBJECT_ID', axis=1, inplace=True)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_proba_validation = rf_classifier.predict_proba(features_data)[:, 1]\n",
    "\n",
    "    # Create a DataFrame for the predictions with SUBJECT_ID as the index\n",
    "    predictions_df = pd.DataFrame(y_pred_proba_validation, index=subject_ids, columns=['Death_Probability'])\n",
    "\n",
    "    return predictions_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ba45a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_rf_new_features(data):\n",
    "    X, y = prepare_features_and_labels(data)\n",
    "\n",
    "    rf_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "    # Perform 5-fold cross-validation and obtain predicted probabilities\n",
    "    y_pred_proba_cv = cross_val_predict(rf_classifier, X, y, cv=5, method='predict_proba')[:, 1]\n",
    "\n",
    "    # Calculate the AUC-ROC score using cross-validated predictions\n",
    "    auc_roc_cv = roc_auc_score(y, y_pred_proba_cv)\n",
    "    print(f\"Cross-Validated AUC ROC Score: {auc_roc_cv}\")\n",
    "\n",
    "    # Calculate the ROC curve using cross-validated predictions\n",
    "    fpr, tpr, thresholds = roc_curve(y, y_pred_proba_cv)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Plot the ROC curve\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    # Train the classifier on the entire dataset for feature importance\n",
    "    rf_classifier.fit(X, y)\n",
    "\n",
    "    # Extract and plot feature importances\n",
    "    feature_importances = rf_classifier.feature_importances_\n",
    "    importances = pd.Series(feature_importances, index=X.columns)\n",
    "    sorted_importances = importances.sort_values(ascending=False)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sorted_importances.plot(kind='bar')\n",
    "    plt.title('Feature Importance')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Importance')\n",
    "    plt.show()\n",
    "    \n",
    "    return rf_classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1214bd5b",
   "metadata": {},
   "source": [
    "# Code for workflow 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21df668c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_feature_selection(data, top_key_players):\n",
    "    \n",
    "    \n",
    "# Recreate the pivot table for the development dataset\n",
    "    pivot_data = data.pivot_table(index='SUBJECT_ID', columns='DEFINITION_ID', values='TIME', aggfunc='first')\n",
    "\n",
    "\n",
    "# Correctly identify the 'death' entries within the 'DEFINITION_ID' column and assign the binary label\n",
    "# '1' for death within a year and '0' for survival\n",
    "    death_events = data[data['DEFINITION_ID'] == 'death']['SUBJECT_ID'].unique()\n",
    "    pivot_data['death'] = pivot_data.index.isin(death_events).astype(int)\n",
    "    \n",
    "    \n",
    "\n",
    "# Drop the 'death' column from features\n",
    "    pivot_data_features = pivot_data.drop('death', axis=1)\n",
    "    pivot_data_label = pivot_data['death']\n",
    "    \n",
    "# We will create a new dataframe using only the key features identified by the graph analysis\n",
    "    key_features = [feature for feature, centrality in top_key_players]\n",
    "\n",
    "# Filter the pivot table to include only key features for model training\n",
    "\n",
    "    key_features_data = pivot_data_features[key_features]\n",
    "\n",
    "# Since we have NaN values (where interventions did not occur), we will impute these with 0\n",
    "# indicating that the intervention did not happen for that patient\n",
    "\n",
    "    imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "\n",
    "    key_features_data_imputed = imputer.fit_transform(key_features_data)\n",
    "\n",
    "# Scale the feature data\n",
    "    scaler = StandardScaler()\n",
    "    key_features_data_scaled = scaler.fit_transform(key_features_data_imputed)\n",
    "    \n",
    "    return key_features_data_scaled, pivot_data_label, key_features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03cf425c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_rf_with_feature_selection_graph(data):\n",
    "    data=pd.read_csv(data)\n",
    "    # Sample a subset of the data to create a smaller graph for analysis\n",
    "    # Since the data may be ordered by SUBJECT_ID, we shuffle the data to get a random sample\n",
    "    data_sample = data.sample(frac=1, random_state=1)\n",
    "    data_sample.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Let's take a sample of 20% of the data to make the graph computation tractable\n",
    "    sample_size = int(0.2 * len(data_sample))\n",
    "    data_sample = data_sample.head(sample_size)\n",
    "    \n",
    "    # Create a new graph from the sampled data\n",
    "    G_sample = nx.Graph()\n",
    "    \n",
    "    # Add nodes and edges from the sampled dataset\n",
    "    for index, row in data_sample.iterrows():\n",
    "        # Add node if it doesn't exist\n",
    "        if not G_sample.has_node(row['DEFINITION_ID']):\n",
    "            G_sample.add_node(row['DEFINITION_ID'])\n",
    "            \n",
    "        # Since the data is ordered by time for each SUBJECT_ID, we can link sequential interventions\n",
    "        if index > 0 and data_sample.iloc[index - 1]['SUBJECT_ID'] == row['SUBJECT_ID']:\n",
    "            prev_definition_id = data_sample.iloc[index - 1]['DEFINITION_ID']\n",
    "            # If edge does not exist, add with weight 1, otherwise increase weight\n",
    "            if not G_sample.has_edge(prev_definition_id, row['DEFINITION_ID']):\n",
    "                G_sample.add_edge(prev_definition_id, row['DEFINITION_ID'], weight=1)\n",
    "            else:\n",
    "                G_sample[prev_definition_id][row['DEFINITION_ID']]['weight'] += 1\n",
    "    # Calculate centrality measures on the sample graph again\n",
    "    degree_centrality_sample = nx.degree_centrality(G_sample)\n",
    "    betweenness_centrality_sample = nx.betweenness_centrality(G_sample)\n",
    "    closeness_centrality_sample = nx.closeness_centrality(G_sample)\n",
    "    \n",
    "    # Identify key players using degree centrality from the sample graph graph\n",
    "    key_players_by_degree = sorted(degree_centrality_sample.items(), key=lambda item: item[1], reverse=True)\n",
    "    top_key_players = key_players_by_degree[:100]\n",
    "    \n",
    "    \n",
    "    X,y,key_features = prepare_data_feature_selection(data, top_key_players)    \n",
    "    \n",
    "    # Initialize the Random Forest Classifier\n",
    "    rf_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "   # Perform 5-fold cross-validation and obtain predicted probabilities\n",
    "    y_pred_proba_cv = cross_val_predict(rf_classifier, X, y, cv=5, method='predict_proba')[:, 1]\n",
    "\n",
    "    # Calculate the AUC-ROC score using cross-validated predictions\n",
    "    auc_roc_cv = roc_auc_score(y, y_pred_proba_cv)\n",
    "    print(f\"Cross-Validated AUC ROC Score: {auc_roc_cv}\")\n",
    "\n",
    "    # Calculate the ROC curve using cross-validated predictions\n",
    "    fpr, tpr, thresholds = roc_curve(y, y_pred_proba_cv)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Plot the ROC curve\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    # Train the classifier on the entire dataset for feature importance\n",
    "    rf_classifier.fit(X, y)\n",
    "\n",
    "    # Extract and plot feature importances\n",
    "    # Extract feature importances from the trained RandomForest model\n",
    "    feature_importances = rf_classifier.feature_importances_\n",
    "    # Create a series with feature names and their importance scores\n",
    "    importances = pd.Series(feature_importances, index=key_features)\n",
    "    # Sort the feature importances in descending order\n",
    "    sorted_importances = importances.sort_values(ascending=False)\n",
    "    # Rename the index and the series for clarity\n",
    "    sorted_importances.index.name = 'definition_id'\n",
    "    sorted_importances.name = 'importance'\n",
    "    # Set option to display all rows of the series\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    # Print the series\n",
    "    print(sorted_importances.to_frame())\n",
    "    \n",
    "    return rf_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4855b717",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X, y = prepare_features_and_labels(data)\n",
    "#X_validation, y_validation = prepare_features_and_labels(validation)\n",
    "\n",
    "# Define the parameter grid to search\n",
    "#param_grid = {\n",
    " #   'n_estimators': [100, 200, 300],  # Number of trees in the forest\n",
    " #   'max_depth': [None, 10, 20, 30],  # Maximum depth of the tree\n",
    " #   'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node\n",
    " #   'min_samples_leaf': [1, 2, 4],    # Minimum number of samples required to be at a leaf node\n",
    "    # Add other parameters here if you want\n",
    "#}\n",
    "\n",
    "# Initialize the RandomForestClassifier\n",
    "#rf_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV with the classifier, parameter grid, and desired scoring metric\n",
    "#grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, \n",
    "                       #    cv=5, n_jobs=-1, scoring='roc_auc', verbose=2)\n",
    "\n",
    "# Fit GridSearchCV to the training data\n",
    "#grid_search.fit(X, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5aa3eb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare the features and labels\n",
    "#X, y = prepare_features_and_labels(data)\n",
    "#X_validation, y_validation = prepare_features_and_labels(validation)\n",
    "\n",
    "# Define the parameter distribution to sample from\n",
    "#param_dist = {\n",
    " #   'n_estimators': [50, 150, 250, 350, 450],  # Extended range of trees in the forest\n",
    " #   'max_depth': [None, 5, 15, 25, 35, 45],    # Extended range for maximum depth of the tree\n",
    " #   'min_samples_split': [2, 5, 7, 15, 20],    # Different values for min samples to split\n",
    " #   'min_samples_leaf': [1, 2, 4, 6, 8],        # Different values for min samples at a leaf\n",
    " #   'bootstrap': [True, False],                 # Whether bootstrap samples are used\n",
    " #   'max_features': [None, 'sqrt', 'log2']    # Number of features to consider at every split\n",
    "    # You can add more parameters here\n",
    "#}\n",
    "\n",
    "# Initialize the RandomForestClassifier\n",
    "#rf_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Initialize RandomizedSearchCV with the classifier, parameter distribution, and desired scoring metric\n",
    "#random_search = RandomizedSearchCV(estimator=rf_classifier, param_distributions=param_dist, \n",
    " #                                  n_iter=100, cv=5, n_jobs=-1, scoring='roc_auc', \n",
    "#                                 random_state=42, verbose=2)\n",
    "\n",
    "# Fit RandomizedSearchCV to the training data\n",
    "#random_search.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "071570e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best estimator\n",
    "#best_rf_classifier = random_search.best_estimator_\n",
    "\n",
    "# Optionally, evaluate on the validation set\n",
    "#X_validation = X_validation.reindex(columns=X.columns, fill_value=0)  # Ensuring column match\n",
    "#y_pred_proba_validation = best_rf_classifier.predict_proba(X_validation)[:, 1]\n",
    "#auc_roc_validation = roc_auc_score(y_validation, y_pred_proba_validation)\n",
    "\n",
    "#print(\"Best Parameters:\", random_search.best_params_)\n",
    "#print(\"Best AUC-ROC on Validation:\", auc_roc_validation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f647be8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute precision and recall values\n",
    "#precision, recall, _ = precision_recall_curve(y_validation, y_pred_proba_validation)\n",
    "\n",
    "# Compute the area under the precision-recall curve\n",
    "#auc_score = auc(recall, precision)\n",
    "\n",
    "# Plotting the Precision-Recall curve\n",
    "#plt.figure(figsize=(8, 6))\n",
    "#plt.plot(recall, precision, color='darkorange', lw=2, label='Precision-Recall curve (area = %0.2f)' % auc_score)\n",
    "#plt.xlabel('Recall')\n",
    "#plt.ylabel('Precision')\n",
    "#plt.title('Precision-Recall Curve')\n",
    "#plt.legend(loc=\"lower left\")\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a70251f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the calibration curve\n",
    "#prob_true, prob_pred = calibration_curve(y_validation, y_pred_proba_validation, n_bins=10, strategy='uniform')\n",
    "\n",
    "# Plotting the Calibration curve\n",
    "#plt.figure(figsize=(8, 6))\n",
    "#plt.plot(prob_pred, prob_true, marker='o', linewidth=1, label='Calibration plot')\n",
    "#plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfectly calibrated')\n",
    "#plt.xlabel('Mean predicted probability')\n",
    "#plt.ylabel('Fraction of positives')\n",
    "#plt.title('Calibration Curve')\n",
    "#plt.legend()\n",
    "#plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
