{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2772d0a",
   "metadata": {},
   "source": [
    "# Data science project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f24bc2",
   "metadata": {},
   "source": [
    "Saadan teile failid .zip kujul. CSV failides on kaks andmetabelit, mis koosnevad sÃ¼nteetilitest andmetest, mis pÃµhinevad reaalsete vÃ¤hihaigete (kopsu ja eesnÃ¤Ã¤rme) haigustrajektooridel. Andmetabelitel on kolm tunnust SUBJECT_ID - unikaalne patsiendi id; DEFINITION_ID - meditsiiniline sekkumine, mis patsiendiga toimus; TIME - aeg aastates, millal sekkumine toimus. Ajal 0 sai iga patsient vÃ¤hidiagnoosi. Osadel patsientidest on olemas seisund \"death\", mis tÃ¤hendab, et see patsient suri antud ajahetkel. KÃµikide patsientide puhul on kustutatud aasta enne suremist/viimast Ã¼leskirjutist.\n",
    "\n",
    "Teie Ã¼lesanne on ennustada patsientide suremust 1-aasta kÃ¤igus (mil andmed lÃµppevad), ehk siis klassifitseerimisÃ¼lesanne, kus klass 1 - patsient sureb; 0 - patsient jÃ¤Ã¤b aasta jooksul ellu. Ãœks viis andmeid tÃ¶Ã¶delda, selleks, et neid mudelitele ette sÃ¶Ã¶ta, on iga unikaalne sekkumine ja selle jÃ¤rjekorra number muuta eraldi tunnuseks, tekitada iga patsiendi kohta Ã¼ks rida ja vÃ¤Ã¤rtuseks vÃµtta aeg. Kuid igasugune andmete tÃ¶Ã¶tlemine on teie enda vaba valik. Te vÃµite tekitada klassifitseerija Ã¼kskÃµik millisel meetodil: otsustuspuud, nÃ¤rvivÃµrgud, logistiline regressioon vms. \n",
    "\n",
    "Ãœks peamine probleem terviseandmetega on tÃµsisasi, et need on Ã¼sna mÃ¼rased. Erinevate sekkumiste all on peidetud definitsioonid, millest osad on arvatavasti ennustamiseks olulised (nÃ¤iteks \"keemiaravi\") ja teised mitte nii vÃ¤ga (nÃ¤iteks \"nÃ¤gemiskontroll\"). MÃ¼raste andmete/tunnuste eemaldamine vÃµib antud Ã¼lesande puhul suuresti tÃµsta mudeli tÃ¤psust. KÃ¤esoleva Ã¼lesande juures on tÃ¤htis, et te ei vÃµi eeldada, et sekkumiste definitsioonid on erinevate andmetabelite seas sama tÃ¤hendusega (antud juhul nÃ¤iteks, et Drug_1 on sama, mis Drug_1 teises andmestikus).\n",
    "\n",
    "TÃ¶Ã¶ tulemuseks peaks olema tÃ¶Ã¶voog, mis vÃµtab samasuguse andmetabeli sisendiks ning vÃ¤ljastaks AUC-ROC vÃ¤Ã¤rtuse ja mudeli enda. Seda vÃµib teha nii jupyter-notebookis kui ka skripti kujul.\n",
    "\n",
    "ZIP failist leiate kaks andmetabelit, vÃµite Ã¼ht kasutada tÃ¶Ã¶voo loomiseks ja teist selleks, et seda valideerida. Andmed on sÃ¼nteetilised ja pole Ã¼ksteisega seotud (SUBJECT_ID vÃ¤Ã¤rtus 1 Ã¼hes ei tÃ¤henda, et tegu on sama inimesega teises andmetabelis). Mul endal on ligipÃ¤Ã¤s pÃ¤ris andmetele, mida ma teiega kahjuks jagada ei saa, aga ma vÃµin jooksutada teie tÃ¶Ã¶voogu pÃ¤ris andmetel ja jagada teiega tulemusi. Te vÃµite vÃ¤ljundisse lisada erinevat statistikat, mida soovite pÃ¤risandmetelt saada. Need vÃ¤ljundid peavad olema summeeritud, mis tÃ¤hendab, et ma ei saa teiega jagada patsiendi tasemel andmeid.\n",
    "\n",
    "Ma ise loodan, et teie loodud tÃ¶Ã¶voog selekteeriks tunnuseid. Parim terviseandmete mudel on vÃµimalikult vÃ¤ikese arvu tunnustega, samuti teades neid tunnuseid saab edendada teadustÃ¶Ã¶d nii haiguste kui ka nende Ã¤rahoidmise/ravimise kohta. Samuti soovitan julgelt proovida graafi loomist ning sellelt \"key-playerite\" jms leidmist, see oleks innovaatiline viis tunnuste valimiseks.\n",
    "\n",
    "FYI, neid samu andmeid kasutades sain ma juhumetsaga AUCROC'i 0.65 ja pÃ¤rast tunnuste selekteerimist oli AUCROC 0.75. Juhul kui tÃ¶Ã¶ tulemus on hea ja kasutatav ka muude haiguste ennustusmudelite loomisel on vÃµimalus saada oma nimi teadusartiklile ðŸ™‚ \n",
    "\n",
    " Hello!\n",
    "\n",
    "I was contacted regarding the concern about death times occurring later than a year from last medical intervention. The problem is present because of my coding error ðŸ™‚ This is okay and does not mess up the overall purpose of your workflow. You can consider all persons who have the \"death\"  state as dying within one year.\n",
    "\n",
    "Sorry about the confusion,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e90af1",
   "metadata": {},
   "source": [
    "# Lung Cancer Survival Prediction with Synthetic Data\n",
    "\n",
    "Objective:\n",
    "Utilize synthetic medical data to predict 5-year survival rates post-lung cancer diagnosis by effectively selecting key features from patient treatment trajectories.\n",
    "Possible approach:\n",
    "1. Feature Selection: Use algorithms like PCA and Stepwise Selection to identify crucial features.\n",
    "2. Subsequence Analysis: Find frequent subsequences within treatment trajectories for additional predictive value. Effectiveness (performance) of the algorithm is important.\n",
    "3. Use network science approach. Using graphs and extracting key-player nodes as features and using them for prediction.\n",
    "4. Any custom approach.\n",
    "End result:\n",
    "Predictive Model: Develop a workflow that, given patient treatment trajectory data, outputs a 5-year survival prediction. Workflow will be tested on a test set not seen by students and compared with other groups work.\n",
    "Outcome:\n",
    "A concise list of significant features, an understanding of vital treatment subsequences, and a prediction function for stakeholder use. Possibility to get published!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85407f3",
   "metadata": {},
   "source": [
    "# Training a predictive model by creating new features. \n",
    "\n",
    "The method roc_rf_new_features(df) takes in a dataframe and return the corresponding ROC AUC score. The model is a tuned RandomForest. The model is trained on a dataset that has new features created from existing features. The same is done for the dataset df. The method also outputs a ROC graph and a feature importance graph of the random forst\n",
    "\n",
    "The method rf_new_features_survival_prediction(df) takes in a dataframe and returns the prediction for each subject. Values close to 0 imply survival, 1 implies death.\n",
    "\n",
    "#### Make sure the other cells are runned before running the 2 below (uncomment)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "515d7699",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m data_sample\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Create the graph from the sampled data again\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m G_sample \u001b[38;5;241m=\u001b[39m nx\u001b[38;5;241m.\u001b[39mGraph()\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Add nodes and edges from the sampled dataset\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m data_sample\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m# Add node if it doesn't exist\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nx' is not defined"
     ]
    }
   ],
   "source": [
    "#validation=pd.read_csv(\"synthetic_data_pca.csv\")\n",
    "#roc_rf_new_features(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89aaec2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_prediction = validation[(validation['DEFINITION_ID'] != 'death') & (validation['SUBJECT_ID'] == 67)]\n",
    "#rf_new_features_survival_prediction(test_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5090b735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the files\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "\n",
    "data=pd.read_csv(\"synthetic_data_lung_cancer.csv\")\n",
    "validation=pd.read_csv(\"synthetic_data_pca.csv\")  \n",
    "\n",
    "\n",
    "#data,validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb2be3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features_and_labels(df):\n",
    "    \"\"\"\n",
    "    Prepares the features and labels for model training from the dataset.\n",
    "    \n",
    "    Args:\n",
    "    - df: A pandas DataFrame containing the data with 'SUBJECT_ID', 'DEFINITION_ID', and 'TIME' columns.\n",
    "    \n",
    "    Returns:\n",
    "    - X: Features DataFrame ready for training.\n",
    "    - y: Labels Series indicating whether death occurred within a year.\n",
    "    \"\"\"\n",
    "    # Extract general categories and append '_count' only if DEFINITION_ID contains '_'\n",
    "    df['category'] = df.apply(lambda row: row['DEFINITION_ID'].split('_')[0] + '_count' if '_' in row['DEFINITION_ID'] else row['DEFINITION_ID'].split('_')[0], axis=1)\n",
    "\n",
    "    # Step 2: Count occurrences of each category per patient\n",
    "    category_counts = df.pivot_table(index='SUBJECT_ID', columns='category', aggfunc='size', fill_value=0)\n",
    "\n",
    "    # Step 3: Calculate time-based features\n",
    "    df['time_of_first_intervention'] = df.groupby('SUBJECT_ID')['TIME'].transform('min')\n",
    "    df['time_since_last_intervention'] = df.groupby('SUBJECT_ID')['TIME'].transform('max')\n",
    "    df['duration_of_treatment'] = df['time_since_last_intervention'] - df['time_of_first_intervention']\n",
    "    df['average_time_between_interventions'] = df.groupby('SUBJECT_ID')['TIME'].transform(lambda x: x.diff().mean() if len(x) > 1 else 0)\n",
    "\n",
    "    # Aggregate these features into a single dataframe, ensuring one row per patient\n",
    "    time_features = df.groupby('SUBJECT_ID')[['time_of_first_intervention', 'time_since_last_intervention', \n",
    "                                              'duration_of_treatment', 'average_time_between_interventions']].first().reset_index()\n",
    "\n",
    "    # Combine counts of categories with time-based features\n",
    "    features_data = pd.merge(category_counts, time_features, on='SUBJECT_ID')\n",
    "\n",
    "    # Step 4: Identify 'death' events and create the binary target variable\n",
    "    death_events = df[df['DEFINITION_ID'] == 'death']['SUBJECT_ID'].unique()\n",
    "    features_data['death'] = features_data['SUBJECT_ID'].isin(death_events).astype(int)\n",
    "\n",
    "    # Drop 'SUBJECT_ID' as it is not a feature\n",
    "    features_data.drop('SUBJECT_ID', axis=1, inplace=True)\n",
    "\n",
    "    # Split the combined data into features and labels\n",
    "    X = features_data.drop('death', axis=1)\n",
    "    y = features_data['death']\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def rf_new_features_survival_prediction(df):\n",
    "    # Extract general categories from DEFINITION_ID and append '_count'\n",
    "    df['category'] = df.apply(lambda row: row['DEFINITION_ID'].split('_')[0] + '_count' if '_' in row['DEFINITION_ID'] else row['DEFINITION_ID'].split('_')[0], axis=1)\n",
    "    # Step 2: Count occurrences of each category per patient\n",
    "    category_counts = df.pivot_table(index='SUBJECT_ID', columns='category', aggfunc='size', fill_value=0)\n",
    "     # Step 3: Calculate time-based features\n",
    "    df['time_of_first_intervention'] = df.groupby('SUBJECT_ID')['TIME'].transform('min')\n",
    "    df['time_since_last_intervention'] = df.groupby('SUBJECT_ID')['TIME'].transform('max')\n",
    "    df['duration_of_treatment'] = df['time_since_last_intervention'] - df['time_of_first_intervention']\n",
    "    df['average_time_between_interventions'] = df.groupby('SUBJECT_ID')['TIME'].transform(lambda x: x.diff().mean() if len(x) > 1 else 0)\n",
    "\n",
    "    # Aggregate these features into a single dataframe, ensuring one row per patient\n",
    "    time_features = df.groupby('SUBJECT_ID')[['time_of_first_intervention', 'time_since_last_intervention', \n",
    "                                              'duration_of_treatment', 'average_time_between_interventions']].first().reset_index()\n",
    "        # Combine counts of categories with time-based features\n",
    "    features_data = pd.merge(category_counts, time_features, on='SUBJECT_ID')\n",
    "    \n",
    "    # Keep 'SUBJECT_ID' for indexing\n",
    "    subject_ids = features_data['SUBJECT_ID']\n",
    "\n",
    "    # Drop 'SUBJECT_ID' as it is not a feature for prediction\n",
    "    features_data.drop('SUBJECT_ID', axis=1, inplace=True)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred_proba_validation = rf_classifier.predict_proba(features_data)[:, 1]\n",
    "\n",
    "    # Create a DataFrame for the predictions with SUBJECT_ID as the index\n",
    "    predictions_df = pd.DataFrame(y_pred_proba_validation, index=subject_ids, columns=['Death_Probability'])\n",
    "\n",
    "    return predictions_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00fe36b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(max_depth=20, min_samples_split=20, n_estimators=200,\n",
       "                       random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_depth=20, min_samples_split=20, n_estimators=200,\n",
       "                       random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(max_depth=20, min_samples_split=20, n_estimators=200,\n",
       "                       random_state=42)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = prepare_features_and_labels(data)\n",
    "\n",
    "rf_classifier = RandomForestClassifier(random_state=42, n_estimators= 200, min_samples_split= 20, min_samples_leaf= 1, max_features='sqrt', max_depth = 20, bootstrap= True)\n",
    "rf_classifier.fit(X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf8ba8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_rf_new_features(validation):\n",
    "    X_validation, y_validation = prepare_features_and_labels(validation)\n",
    "    # Ensure that the columns in the validation set match the training set\n",
    "    X_validation = X_validation.reindex(columns=X.columns, fill_value=0)\n",
    "    \n",
    "    # Predict the probabilities on the validation set\n",
    "    y_pred_proba_validation = rf_classifier.predict_proba(X_validation)[:, 1]\n",
    "\n",
    "    # Calculate the AUC-ROC score on the validation set\n",
    "    auc_roc_validation = roc_auc_score(y_validation, y_pred_proba_validation)\n",
    "    \n",
    "    print(f\"AUC ROC Score {auc_roc_validation}\")\n",
    "    \n",
    "    # Assuming y_validation and y_pred_proba_validation are already defined as shown in your code\n",
    "\n",
    "    # Calculate the ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_validation, y_pred_proba_validation)\n",
    "\n",
    "    # Calculate the AUC\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Plot the ROC curve\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    # Extract feature importances from the trained RandomForest model\n",
    "    feature_importances = rf_classifier.feature_importances_\n",
    "\n",
    "    # Create a series with feature names and their importance scores\n",
    "    importances = pd.Series(feature_importances, index=X.columns)\n",
    "\n",
    "    # Sort the feature importances in descending order\n",
    "    sorted_importances = importances.sort_values(ascending=False)\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sorted_importances.plot(kind='bar')\n",
    "    plt.title('Feature Importance')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Importance')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1214bd5b",
   "metadata": {},
   "source": [
    "# Old cells when we tried to use feature selecetion."
   ]
  },
  {
   "cell_type": "raw",
   "id": "6182273d",
   "metadata": {},
   "source": [
    "# Sample a subset of the data to create a smaller graph for analysis\n",
    "import networkx as nx\n",
    "\n",
    "# Since the data may be ordered by SUBJECT_ID, we shuffle the data to get a random sample\n",
    "data_sample = data.sample(frac=1, random_state=1)\n",
    "data_sample.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Let's take a sample of 50% of the data to make the graph computation tractable\n",
    "sample_size = int(0.5 * len(data_sample))\n",
    "data_sample = data_sample.head(sample_size)\n",
    "\n",
    "# Create a new graph from the sampled data\n",
    "G_sample = nx.Graph()\n",
    "\n",
    "# Add nodes and edges from the sampled dataset\n",
    "for index, row in data_sample.iterrows():\n",
    "    # Add node if it doesn't exist\n",
    "    if not G_sample.has_node(row['DEFINITION_ID']):\n",
    "        G_sample.add_node(row['DEFINITION_ID'])\n",
    "        \n",
    "    # Since the data is ordered by time for each SUBJECT_ID, we can link sequential interventions\n",
    "    if index > 0 and data_sample.iloc[index - 1]['SUBJECT_ID'] == row['SUBJECT_ID']:\n",
    "        prev_definition_id = data_sample.iloc[index - 1]['DEFINITION_ID']\n",
    "        # If edge does not exist, add with weight 1, otherwise increase weight\n",
    "        if not G_sample.has_edge(prev_definition_id, row['DEFINITION_ID']):\n",
    "            G_sample.add_edge(prev_definition_id, row['DEFINITION_ID'], weight=1)\n",
    "        else:\n",
    "            G_sample[prev_definition_id][row['DEFINITION_ID']]['weight'] += 1\n",
    "# Calculate centrality measures on the sample graph again\n",
    "degree_centrality_sample = nx.degree_centrality(G_sample)\n",
    "betweenness_centrality_sample = nx.betweenness_centrality(G_sample)\n",
    "closeness_centrality_sample = nx.closeness_centrality(G_sample)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "90b8dc0d",
   "metadata": {},
   "source": [
    "# Identify key players using degree centrality from the sample graph graph\n",
    "key_players_by_degree = sorted(degree_centrality_sample.items(), key=lambda item: item[1], reverse=True)\n",
    "top_key_players = key_players_by_degree[:75]\n",
    "\n",
    "top_key_players"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1732452d",
   "metadata": {},
   "source": [
    "def prepare_data_feature_selection(data, top_key_players):\n",
    "\n",
    "# Recreate the pivot table for the development dataset\n",
    "    pivot_data = data.pivot_table(index='SUBJECT_ID', columns='DEFINITION_ID', values='TIME', aggfunc='first')\n",
    "\n",
    "\n",
    "# Correctly identify the 'death' entries within the 'DEFINITION_ID' column and assign the binary label\n",
    "# '1' for death within a year and '0' for survival\n",
    "    death_events = data[data['DEFINITION_ID'] == 'death']['SUBJECT_ID'].unique()\n",
    "    pivot_data['death'] = pivot_data.index.isin(death_events).astype(int)\n",
    "    \n",
    "    \n",
    "\n",
    "# Drop the 'death' column from features\n",
    "    pivot_data_features = pivot_data.drop('death', axis=1)\n",
    "    pivot_data_label = pivot_data['death']\n",
    "    \n",
    "# We will create a new dataframe using only the key features identified by the graph analysis\n",
    "    key_features = [feature for feature, centrality in top_key_players]\n",
    "\n",
    "# Filter the pivot table to include only key features for model training\n",
    "\n",
    "    key_features_data = pivot_data_features[key_features]\n",
    "\n",
    "# Since we have NaN values (where interventions did not occur), we will impute these with 0\n",
    "# indicating that the intervention did not happen for that patient\n",
    "\n",
    "    imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "\n",
    "    key_features_data_imputed = imputer.fit_transform(key_features_data)\n",
    "\n",
    "# Scale the feature data\n",
    "    scaler = StandardScaler()\n",
    "    key_features_data_scaled = scaler.fit_transform(key_features_data_imputed)\n",
    "    \n",
    "    return key_features_data_scaled, pivot_data_label\n",
    "\n",
    "\n",
    "\n",
    "train_X, train_y = prepare_data_feature_selection(data, top_key_players)\n",
    "val_X, val_y = prepare_data_feature_selection(validation, top_key_players)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a853396f",
   "metadata": {},
   "source": [
    "# Initialize the Random Forest Classifier\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Train the Random Forest Classifier\n",
    "rf.fit(train_X, train_y)\n",
    "# Predict probabilities for the test set\n",
    "y_pred_proba_rf = rf.predict_proba(val_X)[:, 1]\n",
    "\n",
    "# Calculate the AUC-ROC score for the test set using the Random Forest model trained on SMOTE data\n",
    "auc_roc_rf= roc_auc_score(val_y, y_pred_proba_rf)\n",
    "\n",
    "auc_roc_rf"
   ]
  },
  {
   "cell_type": "raw",
   "id": "74cc7fb0",
   "metadata": {},
   "source": [
    "# Extract feature importances from the trained RandomForest model\n",
    "feature_importances = rf.feature_importances_\n",
    "\n",
    "# Create a series with feature names and their importance scores\n",
    "importances = pd.Series(feature_importances, index=top_key_players)\n",
    "\n",
    "# Sort the feature importances in descending order\n",
    "sorted_importances = importances.sort_values(ascending=False)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10,6))\n",
    "sorted_importances.plot(kind='bar')\n",
    "plt.title('Feature Importance')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9dcda9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89145409",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X, y = prepare_features_and_labels(data)\n",
    "#X_validation, y_validation = prepare_features_and_labels(validation)\n",
    "\n",
    "# Define the parameter grid to search\n",
    "#param_grid = {\n",
    " #   'n_estimators': [100, 200, 300],  # Number of trees in the forest\n",
    " #   'max_depth': [None, 10, 20, 30],  # Maximum depth of the tree\n",
    " #   'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node\n",
    " #   'min_samples_leaf': [1, 2, 4],    # Minimum number of samples required to be at a leaf node\n",
    "    # Add other parameters here if you want\n",
    "#}\n",
    "\n",
    "# Initialize the RandomForestClassifier\n",
    "#rf_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV with the classifier, parameter grid, and desired scoring metric\n",
    "#grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, \n",
    "                       #    cv=5, n_jobs=-1, scoring='roc_auc', verbose=2)\n",
    "\n",
    "# Fit GridSearchCV to the training data\n",
    "#grid_search.fit(X, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bf5b27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare the features and labels\n",
    "#X, y = prepare_features_and_labels(data)\n",
    "#X_validation, y_validation = prepare_features_and_labels(validation)\n",
    "\n",
    "# Define the parameter distribution to sample from\n",
    "#param_dist = {\n",
    " #   'n_estimators': [50, 150, 250, 350, 450],  # Extended range of trees in the forest\n",
    " #   'max_depth': [None, 5, 15, 25, 35, 45],    # Extended range for maximum depth of the tree\n",
    " #   'min_samples_split': [2, 5, 7, 15, 20],    # Different values for min samples to split\n",
    " #   'min_samples_leaf': [1, 2, 4, 6, 8],        # Different values for min samples at a leaf\n",
    " #   'bootstrap': [True, False],                 # Whether bootstrap samples are used\n",
    " #   'max_features': [None, 'sqrt', 'log2']    # Number of features to consider at every split\n",
    "    # You can add more parameters here\n",
    "#}\n",
    "\n",
    "# Initialize the RandomForestClassifier\n",
    "#rf_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Initialize RandomizedSearchCV with the classifier, parameter distribution, and desired scoring metric\n",
    "#random_search = RandomizedSearchCV(estimator=rf_classifier, param_distributions=param_dist, \n",
    " #                                  n_iter=100, cv=5, n_jobs=-1, scoring='roc_auc', \n",
    "#                                 random_state=42, verbose=2)\n",
    "\n",
    "# Fit RandomizedSearchCV to the training data\n",
    "#random_search.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d5ed467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best estimator\n",
    "#best_rf_classifier = random_search.best_estimator_\n",
    "\n",
    "# Optionally, evaluate on the validation set\n",
    "#X_validation = X_validation.reindex(columns=X.columns, fill_value=0)  # Ensuring column match\n",
    "#y_pred_proba_validation = best_rf_classifier.predict_proba(X_validation)[:, 1]\n",
    "#auc_roc_validation = roc_auc_score(y_validation, y_pred_proba_validation)\n",
    "\n",
    "#print(\"Best Parameters:\", random_search.best_params_)\n",
    "#print(\"Best AUC-ROC on Validation:\", auc_roc_validation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebb119f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute precision and recall values\n",
    "#precision, recall, _ = precision_recall_curve(y_validation, y_pred_proba_validation)\n",
    "\n",
    "# Compute the area under the precision-recall curve\n",
    "#auc_score = auc(recall, precision)\n",
    "\n",
    "# Plotting the Precision-Recall curve\n",
    "#plt.figure(figsize=(8, 6))\n",
    "#plt.plot(recall, precision, color='darkorange', lw=2, label='Precision-Recall curve (area = %0.2f)' % auc_score)\n",
    "#plt.xlabel('Recall')\n",
    "#plt.ylabel('Precision')\n",
    "#plt.title('Precision-Recall Curve')\n",
    "#plt.legend(loc=\"lower left\")\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a9bbef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the calibration curve\n",
    "#prob_true, prob_pred = calibration_curve(y_validation, y_pred_proba_validation, n_bins=10, strategy='uniform')\n",
    "\n",
    "# Plotting the Calibration curve\n",
    "#plt.figure(figsize=(8, 6))\n",
    "#plt.plot(prob_pred, prob_true, marker='o', linewidth=1, label='Calibration plot')\n",
    "#plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfectly calibrated')\n",
    "#plt.xlabel('Mean predicted probability')\n",
    "#plt.ylabel('Fraction of positives')\n",
    "#plt.title('Calibration Curve')\n",
    "#plt.legend()\n",
    "#plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
