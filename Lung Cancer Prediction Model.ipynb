{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2772d0a",
   "metadata": {},
   "source": [
    "# Data science project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f24bc2",
   "metadata": {},
   "source": [
    "Saadan teile failid .zip kujul. CSV failides on kaks andmetabelit, mis koosnevad s√ºnteetilitest andmetest, mis p√µhinevad reaalsete v√§hihaigete (kopsu ja eesn√§√§rme) haigustrajektooridel. Andmetabelitel on kolm tunnust SUBJECT_ID - unikaalne patsiendi id; DEFINITION_ID - meditsiiniline sekkumine, mis patsiendiga toimus; TIME - aeg aastates, millal sekkumine toimus. Ajal 0 sai iga patsient v√§hidiagnoosi. Osadel patsientidest on olemas seisund \"death\", mis t√§hendab, et see patsient suri antud ajahetkel. K√µikide patsientide puhul on kustutatud aasta enne suremist/viimast √ºleskirjutist.\n",
    "\n",
    "Teie √ºlesanne on ennustada patsientide suremust 1-aasta k√§igus (mil andmed l√µppevad), ehk siis klassifitseerimis√ºlesanne, kus klass 1 - patsient sureb; 0 - patsient j√§√§b aasta jooksul ellu. √úks viis andmeid t√∂√∂delda, selleks, et neid mudelitele ette s√∂√∂ta, on iga unikaalne sekkumine ja selle j√§rjekorra number muuta eraldi tunnuseks, tekitada iga patsiendi kohta √ºks rida ja v√§√§rtuseks v√µtta aeg. Kuid igasugune andmete t√∂√∂tlemine on teie enda vaba valik. Te v√µite tekitada klassifitseerija √ºksk√µik millisel meetodil: otsustuspuud, n√§rviv√µrgud, logistiline regressioon vms. \n",
    "\n",
    "√úks peamine probleem terviseandmetega on t√µsisasi, et need on √ºsna m√ºrased. Erinevate sekkumiste all on peidetud definitsioonid, millest osad on arvatavasti ennustamiseks olulised (n√§iteks \"keemiaravi\") ja teised mitte nii v√§ga (n√§iteks \"n√§gemiskontroll\"). M√ºraste andmete/tunnuste eemaldamine v√µib antud √ºlesande puhul suuresti t√µsta mudeli t√§psust. K√§esoleva √ºlesande juures on t√§htis, et te ei v√µi eeldada, et sekkumiste definitsioonid on erinevate andmetabelite seas sama t√§hendusega (antud juhul n√§iteks, et Drug_1 on sama, mis Drug_1 teises andmestikus).\n",
    "\n",
    "T√∂√∂ tulemuseks peaks olema t√∂√∂voog, mis v√µtab samasuguse andmetabeli sisendiks ning v√§ljastaks AUC-ROC v√§√§rtuse ja mudeli enda. Seda v√µib teha nii jupyter-notebookis kui ka skripti kujul.\n",
    "\n",
    "ZIP failist leiate kaks andmetabelit, v√µite √ºht kasutada t√∂√∂voo loomiseks ja teist selleks, et seda valideerida. Andmed on s√ºnteetilised ja pole √ºksteisega seotud (SUBJECT_ID v√§√§rtus 1 √ºhes ei t√§henda, et tegu on sama inimesega teises andmetabelis). Mul endal on ligip√§√§s p√§ris andmetele, mida ma teiega kahjuks jagada ei saa, aga ma v√µin jooksutada teie t√∂√∂voogu p√§ris andmetel ja jagada teiega tulemusi. Te v√µite v√§ljundisse lisada erinevat statistikat, mida soovite p√§risandmetelt saada. Need v√§ljundid peavad olema summeeritud, mis t√§hendab, et ma ei saa teiega jagada patsiendi tasemel andmeid.\n",
    "\n",
    "Ma ise loodan, et teie loodud t√∂√∂voog selekteeriks tunnuseid. Parim terviseandmete mudel on v√µimalikult v√§ikese arvu tunnustega, samuti teades neid tunnuseid saab edendada teadust√∂√∂d nii haiguste kui ka nende √§rahoidmise/ravimise kohta. Samuti soovitan julgelt proovida graafi loomist ning sellelt \"key-playerite\" jms leidmist, see oleks innovaatiline viis tunnuste valimiseks.\n",
    "\n",
    "FYI, neid samu andmeid kasutades sain ma juhumetsaga AUCROC'i 0.65 ja p√§rast tunnuste selekteerimist oli AUCROC 0.75. Juhul kui t√∂√∂ tulemus on hea ja kasutatav ka muude haiguste ennustusmudelite loomisel on v√µimalus saada oma nimi teadusartiklile üôÇ \n",
    "\n",
    " Hello!\n",
    "\n",
    "I was contacted regarding the concern about death times occurring later than a year from last medical intervention. The problem is present because of my coding error üôÇ This is okay and does not mess up the overall purpose of your workflow. You can consider all persons who have the \"death\"  state as dying within one year.\n",
    "\n",
    "Sorry about the confusion,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e90af1",
   "metadata": {},
   "source": [
    "# Lung Cancer Survival Prediction with Synthetic Data\n",
    "\n",
    "Objective:\n",
    "Utilize synthetic medical data to predict 5-year survival rates post-lung cancer diagnosis by effectively selecting key features from patient treatment trajectories.\n",
    "Possible approach:\n",
    "1. Feature Selection: Use algorithms like PCA and Stepwise Selection to identify crucial features.\n",
    "2. Subsequence Analysis: Find frequent subsequences within treatment trajectories for additional predictive value. Effectiveness (performance) of the algorithm is important.\n",
    "3. Use network science approach. Using graphs and extracting key-player nodes as features and using them for prediction.\n",
    "4. Any custom approach.\n",
    "End result:\n",
    "Predictive Model: Develop a workflow that, given patient treatment trajectory data, outputs a 5-year survival prediction. Workflow will be tested on a test set not seen by students and compared with other groups work.\n",
    "Outcome:\n",
    "A concise list of significant features, an understanding of vital treatment subsequences, and a prediction function for stakeholder use. Possibility to get published!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5090b735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.5468292618426"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the files\n",
    "import pandas as pd\n",
    "data=pd.read_csv(\"synthetic_data_lung_cancer.csv\")\n",
    "validation=pd.read_csv(\"synthetic_data_pca.csv\")\n",
    "\n",
    "#data,validation\n",
    "max(sorted(validation[\"TIME\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb2be3f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DEFINITION_ID  condition_1  condition_10  condition_100  condition_1000  \\\n",
       " SUBJECT_ID                                                                \n",
       " 1                 2.096877           NaN            NaN             NaN   \n",
       " 2                      NaN           NaN            NaN             NaN   \n",
       " 3                      NaN           NaN            NaN             NaN   \n",
       " 4                      NaN           NaN            NaN             NaN   \n",
       " 6                      NaN           NaN            NaN             NaN   \n",
       " \n",
       " DEFINITION_ID  condition_1001  condition_1002  condition_1003  condition_1004  \\\n",
       " SUBJECT_ID                                                                      \n",
       " 1                         NaN             NaN             NaN             NaN   \n",
       " 2                         NaN             NaN             NaN             NaN   \n",
       " 3                         NaN             NaN             NaN         3.46837   \n",
       " 4                         NaN             NaN             NaN             NaN   \n",
       " 6                         NaN             NaN             NaN             NaN   \n",
       " \n",
       " DEFINITION_ID  condition_1005  condition_1006  ...  procedure_90  \\\n",
       " SUBJECT_ID                                     ...                 \n",
       " 1                         NaN             NaN  ...           NaN   \n",
       " 2                         NaN             NaN  ...           NaN   \n",
       " 3                         NaN             NaN  ...      2.483872   \n",
       " 4                         NaN             NaN  ...           NaN   \n",
       " 6                         NaN             NaN  ...           NaN   \n",
       " \n",
       " DEFINITION_ID  procedure_91  procedure_92  procedure_93  procedure_94  \\\n",
       " SUBJECT_ID                                                              \n",
       " 1                       NaN           NaN           NaN           NaN   \n",
       " 2                       NaN           NaN           NaN           NaN   \n",
       " 3                       NaN      2.525181           NaN           NaN   \n",
       " 4                       NaN           NaN           NaN       2.80732   \n",
       " 6                       NaN           NaN           NaN           NaN   \n",
       " \n",
       " DEFINITION_ID  procedure_95  procedure_96  procedure_97  procedure_98  \\\n",
       " SUBJECT_ID                                                              \n",
       " 1                       NaN           NaN           NaN           NaN   \n",
       " 2                       NaN           NaN           NaN      0.893354   \n",
       " 3                       NaN           NaN           NaN           NaN   \n",
       " 4                       NaN           NaN           NaN           NaN   \n",
       " 6                       NaN           NaN           NaN           NaN   \n",
       " \n",
       " DEFINITION_ID  procedure_99  \n",
       " SUBJECT_ID                   \n",
       " 1                       NaN  \n",
       " 2                       NaN  \n",
       " 3                       NaN  \n",
       " 4                       NaN  \n",
       " 6                       NaN  \n",
       " \n",
       " [5 rows x 4863 columns],\n",
       " 0    464\n",
       " 1    263\n",
       " Name: death, dtype: int64)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Correcting the approach to create the target label 'death'\n",
    "\n",
    "# Recreate the pivot table for the development dataset\n",
    "pivot_data = data.pivot_table(index='SUBJECT_ID', columns='DEFINITION_ID', values='TIME', aggfunc='first')\n",
    "\n",
    "# Correctly identify the 'death' entries within the 'DEFINITION_ID' column and assign the binary label\n",
    "# '1' for death within a year and '0' for survival\n",
    "death_events = data[data['DEFINITION_ID'] == 'death']['SUBJECT_ID'].unique()\n",
    "pivot_data['death'] = pivot_data.index.isin(death_events).astype(int)\n",
    "\n",
    "# Drop the 'death' column from features\n",
    "pivot_data_features = pivot_data.drop('death', axis=1)\n",
    "pivot_data_label = pivot_data['death']\n",
    "\n",
    "# Display the first few rows of the features and the distribution of the label\n",
    "pivot_data_features_head = pivot_data_features.head()\n",
    "pivot_data_label_distribution = pivot_data_label.value_counts()\n",
    "\n",
    "pivot_data_features_head, pivot_data_label_distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "846d980c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m data_sample\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Create the graph from the sampled data again\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m G_sample \u001b[38;5;241m=\u001b[39m nx\u001b[38;5;241m.\u001b[39mGraph()\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Add nodes and edges from the sampled dataset\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m data_sample\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m# Add node if it doesn't exist\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nx' is not defined"
     ]
    }
   ],
   "source": [
    "# Sample a subset of the data to create a smaller graph for analysis\n",
    "\n",
    "# We will take a random sample of patients and their corresponding interventions\n",
    "\n",
    "\n",
    "# Since the data may be ordered by SUBJECT_ID, we shuffle the data to get a random sample\n",
    "\n",
    "data_sample = data.sample(frac=1, random_state=1)\n",
    "\n",
    "\n",
    "# Let's take a sample of 5% of the data to make the graph computation tractable\n",
    "\n",
    "sample_size = int(0.5 * len(data_sample))\n",
    "\n",
    "data_sample = data_sample.head(sample_size)\n",
    "\n",
    "\n",
    "# Reset the index of the sampled data to avoid IndexError during iteration\n",
    "data_sample.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Create the graph from the sampled data again\n",
    "G_sample = nx.Graph()\n",
    "\n",
    "# Add nodes and edges from the sampled dataset\n",
    "for index, row in data_sample.iterrows():\n",
    "    # Add node if it doesn't exist\n",
    "    if not G_sample.has_node(row['DEFINITION_ID']):\n",
    "        G_sample.add_node(row['DEFINITION_ID'])\n",
    "    \n",
    "    # Since the data is ordered by time for each SUBJECT_ID, we can link sequential interventions\n",
    "    if index > 0 and data_sample.iloc[index - 1]['SUBJECT_ID'] == row['SUBJECT_ID']:\n",
    "        prev_definition_id = data_sample.iloc[index - 1]['DEFINITION_ID']\n",
    "        # If edge does not exist, add with weight 1, otherwise increase weight\n",
    "        if not G_sample.has_edge(prev_definition_id, row['DEFINITION_ID']):\n",
    "            G_sample.add_edge(prev_definition_id, row['DEFINITION_ID'], weight=1)\n",
    "        else:\n",
    "            G_sample[prev_definition_id][row['DEFINITION_ID']]['weight'] += 1\n",
    "\n",
    "# Calculate centrality measures on the sample graph again\n",
    "degree_centrality_sample = nx.degree_centrality(G_sample)\n",
    "betweenness_centrality_sample = nx.betweenness_centrality(G_sample)\n",
    "closeness_centrality_sample = nx.closeness_centrality(G_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d398801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify key players using degree centrality from the full graph\n",
    "key_players_by_degree_full = sorted(degree_centrality_full.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "# Retrieve the top 100 key players from the full dataset\n",
    "top_key_players_full = key_players_by_degree_full[:300]\n",
    "\n",
    "top_key_players_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90868493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will now split the data with the corrected labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    key_features_data_scaled, pivot_data_label, test_size=0.2, random_state=42, stratify=pivot_data_label)\n",
    "\n",
    "# Train the logistic regression model using the corrected data\n",
    "logreg = LogisticRegression(max_iter=10000)\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities for the test set\n",
    "y_pred_proba = logreg.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate the AUC-ROC score for the test set\n",
    "auc_roc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "auc_roc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdb1c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Apply SMOTE to the training data only\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Initialize the Random Forest Classifier\n",
    "rf_smote = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Train the Random Forest Classifier on the SMOTE-resampled training data\n",
    "rf_smote.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Predict probabilities for the test set\n",
    "y_pred_proba_rf_smote = rf_smote.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate the AUC-ROC score for the test set using the Random Forest model trained on SMOTE data\n",
    "auc_roc_rf_smote = roc_auc_score(y_test, y_pred_proba_rf_smote)\n",
    "\n",
    "# Output the SMOTE class distribution and the AUC-ROC score\n",
    "smote_class_distribution = pd.Series(y_train_smote).value_counts(), auc_roc_rf_smote\n",
    "\n",
    "auc_roc_rf_smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d8371b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
