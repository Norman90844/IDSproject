{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2772d0a",
   "metadata": {},
   "source": [
    "# Data science project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f24bc2",
   "metadata": {},
   "source": [
    "Saadan teile failid .zip kujul. CSV failides on kaks andmetabelit, mis koosnevad sÃ¼nteetilitest andmetest, mis pÃµhinevad reaalsete vÃ¤hihaigete (kopsu ja eesnÃ¤Ã¤rme) haigustrajektooridel. Andmetabelitel on kolm tunnust SUBJECT_ID - unikaalne patsiendi id; DEFINITION_ID - meditsiiniline sekkumine, mis patsiendiga toimus; TIME - aeg aastates, millal sekkumine toimus. Ajal 0 sai iga patsient vÃ¤hidiagnoosi. Osadel patsientidest on olemas seisund \"death\", mis tÃ¤hendab, et see patsient suri antud ajahetkel. KÃµikide patsientide puhul on kustutatud aasta enne suremist/viimast Ã¼leskirjutist.\n",
    "\n",
    "Teie Ã¼lesanne on ennustada patsientide suremust 1-aasta kÃ¤igus (mil andmed lÃµppevad), ehk siis klassifitseerimisÃ¼lesanne, kus klass 1 - patsient sureb; 0 - patsient jÃ¤Ã¤b aasta jooksul ellu. Ãœks viis andmeid tÃ¶Ã¶delda, selleks, et neid mudelitele ette sÃ¶Ã¶ta, on iga unikaalne sekkumine ja selle jÃ¤rjekorra number muuta eraldi tunnuseks, tekitada iga patsiendi kohta Ã¼ks rida ja vÃ¤Ã¤rtuseks vÃµtta aeg. Kuid igasugune andmete tÃ¶Ã¶tlemine on teie enda vaba valik. Te vÃµite tekitada klassifitseerija Ã¼kskÃµik millisel meetodil: otsustuspuud, nÃ¤rvivÃµrgud, logistiline regressioon vms. \n",
    "\n",
    "Ãœks peamine probleem terviseandmetega on tÃµsisasi, et need on Ã¼sna mÃ¼rased. Erinevate sekkumiste all on peidetud definitsioonid, millest osad on arvatavasti ennustamiseks olulised (nÃ¤iteks \"keemiaravi\") ja teised mitte nii vÃ¤ga (nÃ¤iteks \"nÃ¤gemiskontroll\"). MÃ¼raste andmete/tunnuste eemaldamine vÃµib antud Ã¼lesande puhul suuresti tÃµsta mudeli tÃ¤psust. KÃ¤esoleva Ã¼lesande juures on tÃ¤htis, et te ei vÃµi eeldada, et sekkumiste definitsioonid on erinevate andmetabelite seas sama tÃ¤hendusega (antud juhul nÃ¤iteks, et Drug_1 on sama, mis Drug_1 teises andmestikus).\n",
    "\n",
    "TÃ¶Ã¶ tulemuseks peaks olema tÃ¶Ã¶voog, mis vÃµtab samasuguse andmetabeli sisendiks ning vÃ¤ljastaks AUC-ROC vÃ¤Ã¤rtuse ja mudeli enda. Seda vÃµib teha nii jupyter-notebookis kui ka skripti kujul.\n",
    "\n",
    "ZIP failist leiate kaks andmetabelit, vÃµite Ã¼ht kasutada tÃ¶Ã¶voo loomiseks ja teist selleks, et seda valideerida. Andmed on sÃ¼nteetilised ja pole Ã¼ksteisega seotud (SUBJECT_ID vÃ¤Ã¤rtus 1 Ã¼hes ei tÃ¤henda, et tegu on sama inimesega teises andmetabelis). Mul endal on ligipÃ¤Ã¤s pÃ¤ris andmetele, mida ma teiega kahjuks jagada ei saa, aga ma vÃµin jooksutada teie tÃ¶Ã¶voogu pÃ¤ris andmetel ja jagada teiega tulemusi. Te vÃµite vÃ¤ljundisse lisada erinevat statistikat, mida soovite pÃ¤risandmetelt saada. Need vÃ¤ljundid peavad olema summeeritud, mis tÃ¤hendab, et ma ei saa teiega jagada patsiendi tasemel andmeid.\n",
    "\n",
    "Ma ise loodan, et teie loodud tÃ¶Ã¶voog selekteeriks tunnuseid. Parim terviseandmete mudel on vÃµimalikult vÃ¤ikese arvu tunnustega, samuti teades neid tunnuseid saab edendada teadustÃ¶Ã¶d nii haiguste kui ka nende Ã¤rahoidmise/ravimise kohta. Samuti soovitan julgelt proovida graafi loomist ning sellelt \"key-playerite\" jms leidmist, see oleks innovaatiline viis tunnuste valimiseks.\n",
    "\n",
    "FYI, neid samu andmeid kasutades sain ma juhumetsaga AUCROC'i 0.65 ja pÃ¤rast tunnuste selekteerimist oli AUCROC 0.75. Juhul kui tÃ¶Ã¶ tulemus on hea ja kasutatav ka muude haiguste ennustusmudelite loomisel on vÃµimalus saada oma nimi teadusartiklile ðŸ™‚ \n",
    "\n",
    " Hello!\n",
    "\n",
    "I was contacted regarding the concern about death times occurring later than a year from last medical intervention. The problem is present because of my coding error ðŸ™‚ This is okay and does not mess up the overall purpose of your workflow. You can consider all persons who have the \"death\"  state as dying within one year.\n",
    "\n",
    "Sorry about the confusion,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e90af1",
   "metadata": {},
   "source": [
    "# Lung Cancer Survival Prediction with Synthetic Data\n",
    "\n",
    "Objective:\n",
    "Utilize synthetic medical data to predict 5-year survival rates post-lung cancer diagnosis by effectively selecting key features from patient treatment trajectories.\n",
    "Possible approach:\n",
    "1. Feature Selection: Use algorithms like PCA and Stepwise Selection to identify crucial features.\n",
    "2. Subsequence Analysis: Find frequent subsequences within treatment trajectories for additional predictive value. Effectiveness (performance) of the algorithm is important.\n",
    "3. Use network science approach. Using graphs and extracting key-player nodes as features and using them for prediction.\n",
    "4. Any custom approach.\n",
    "End result:\n",
    "Predictive Model: Develop a workflow that, given patient treatment trajectory data, outputs a 5-year survival prediction. Workflow will be tested on a test set not seen by students and compared with other groups work.\n",
    "Outcome:\n",
    "A concise list of significant features, an understanding of vital treatment subsequences, and a prediction function for stakeholder use. Possibility to get published!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5090b735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.5468292618426"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the files\n",
    "import pandas as pd\n",
    "data=pd.read_csv(\"synthetic_data_lung_cancer.csv\")\n",
    "validation=pd.read_csv(\"synthetic_data_pca.csv\")\n",
    "\n",
    "#data,validation\n",
    "max(sorted(validation[\"TIME\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb2be3f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DEFINITION_ID  condition_1  condition_10  condition_100  condition_1000  \\\n",
       " SUBJECT_ID                                                                \n",
       " 1                 2.096877           NaN            NaN             NaN   \n",
       " 2                      NaN           NaN            NaN             NaN   \n",
       " 3                      NaN           NaN            NaN             NaN   \n",
       " 4                      NaN           NaN            NaN             NaN   \n",
       " 6                      NaN           NaN            NaN             NaN   \n",
       " \n",
       " DEFINITION_ID  condition_1001  condition_1002  condition_1003  condition_1004  \\\n",
       " SUBJECT_ID                                                                      \n",
       " 1                         NaN             NaN             NaN             NaN   \n",
       " 2                         NaN             NaN             NaN             NaN   \n",
       " 3                         NaN             NaN             NaN         3.46837   \n",
       " 4                         NaN             NaN             NaN             NaN   \n",
       " 6                         NaN             NaN             NaN             NaN   \n",
       " \n",
       " DEFINITION_ID  condition_1005  condition_1006  ...  procedure_90  \\\n",
       " SUBJECT_ID                                     ...                 \n",
       " 1                         NaN             NaN  ...           NaN   \n",
       " 2                         NaN             NaN  ...           NaN   \n",
       " 3                         NaN             NaN  ...      2.483872   \n",
       " 4                         NaN             NaN  ...           NaN   \n",
       " 6                         NaN             NaN  ...           NaN   \n",
       " \n",
       " DEFINITION_ID  procedure_91  procedure_92  procedure_93  procedure_94  \\\n",
       " SUBJECT_ID                                                              \n",
       " 1                       NaN           NaN           NaN           NaN   \n",
       " 2                       NaN           NaN           NaN           NaN   \n",
       " 3                       NaN      2.525181           NaN           NaN   \n",
       " 4                       NaN           NaN           NaN       2.80732   \n",
       " 6                       NaN           NaN           NaN           NaN   \n",
       " \n",
       " DEFINITION_ID  procedure_95  procedure_96  procedure_97  procedure_98  \\\n",
       " SUBJECT_ID                                                              \n",
       " 1                       NaN           NaN           NaN           NaN   \n",
       " 2                       NaN           NaN           NaN      0.893354   \n",
       " 3                       NaN           NaN           NaN           NaN   \n",
       " 4                       NaN           NaN           NaN           NaN   \n",
       " 6                       NaN           NaN           NaN           NaN   \n",
       " \n",
       " DEFINITION_ID  procedure_99  \n",
       " SUBJECT_ID                   \n",
       " 1                       NaN  \n",
       " 2                       NaN  \n",
       " 3                       NaN  \n",
       " 4                       NaN  \n",
       " 6                       NaN  \n",
       " \n",
       " [5 rows x 4863 columns],\n",
       " 0    464\n",
       " 1    263\n",
       " Name: death, dtype: int64)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Correcting the approach to create the target label 'death'\n",
    "\n",
    "# Recreate the pivot table for the development dataset\n",
    "pivot_data = data.pivot_table(index='SUBJECT_ID', columns='DEFINITION_ID', values='TIME', aggfunc='first')\n",
    "\n",
    "# Correctly identify the 'death' entries within the 'DEFINITION_ID' column and assign the binary label\n",
    "# '1' for death within a year and '0' for survival\n",
    "death_events = data[data['DEFINITION_ID'] == 'death']['SUBJECT_ID'].unique()\n",
    "pivot_data['death'] = pivot_data.index.isin(death_events).astype(int)\n",
    "\n",
    "# Drop the 'death' column from features\n",
    "pivot_data_features = pivot_data.drop('death', axis=1)\n",
    "pivot_data_label = pivot_data['death']\n",
    "\n",
    "# Display the first few rows of the features and the distribution of the label\n",
    "pivot_data_features_head = pivot_data_features.head()\n",
    "pivot_data_label_distribution = pivot_data_label.value_counts()\n",
    "\n",
    "pivot_data_features_head, pivot_data_label_distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "846d980c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m data_sample\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Create the graph from the sampled data again\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m G_sample \u001b[38;5;241m=\u001b[39m nx\u001b[38;5;241m.\u001b[39mGraph()\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Add nodes and edges from the sampled dataset\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m data_sample\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m# Add node if it doesn't exist\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nx' is not defined"
     ]
    }
   ],
   "source": [
    "# Sample a subset of the data to create a smaller graph for analysis\n",
    "\n",
    "# We will take a random sample of patients and their corresponding interventions\n",
    "\n",
    "\n",
    "# Since the data may be ordered by SUBJECT_ID, we shuffle the data to get a random sample\n",
    "\n",
    "data_sample = data.sample(frac=1, random_state=1)\n",
    "\n",
    "\n",
    "# Let's take a sample of 5% of the data to make the graph computation tractable\n",
    "\n",
    "sample_size = int(0.5 * len(data_sample))\n",
    "\n",
    "data_sample = data_sample.head(sample_size)\n",
    "\n",
    "\n",
    "# Reset the index of the sampled data to avoid IndexError during iteration\n",
    "data_sample.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Create the graph from the sampled data again\n",
    "G_sample = nx.Graph()\n",
    "\n",
    "# Add nodes and edges from the sampled dataset\n",
    "for index, row in data_sample.iterrows():\n",
    "    # Add node if it doesn't exist\n",
    "    if not G_sample.has_node(row['DEFINITION_ID']):\n",
    "        G_sample.add_node(row['DEFINITION_ID'])\n",
    "    \n",
    "    # Since the data is ordered by time for each SUBJECT_ID, we can link sequential interventions\n",
    "    if index > 0 and data_sample.iloc[index - 1]['SUBJECT_ID'] == row['SUBJECT_ID']:\n",
    "        prev_definition_id = data_sample.iloc[index - 1]['DEFINITION_ID']\n",
    "        # If edge does not exist, add with weight 1, otherwise increase weight\n",
    "        if not G_sample.has_edge(prev_definition_id, row['DEFINITION_ID']):\n",
    "            G_sample.add_edge(prev_definition_id, row['DEFINITION_ID'], weight=1)\n",
    "        else:\n",
    "            G_sample[prev_definition_id][row['DEFINITION_ID']]['weight'] += 1\n",
    "\n",
    "# Calculate centrality measures on the sample graph again\n",
    "degree_centrality_sample = nx.degree_centrality(G_sample)\n",
    "betweenness_centrality_sample = nx.betweenness_centrality(G_sample)\n",
    "closeness_centrality_sample = nx.closeness_centrality(G_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d398801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify key players using degree centrality from the full graph\n",
    "key_players_by_degree_full = sorted(degree_centrality_full.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "# Retrieve the top 100 key players from the full dataset\n",
    "top_key_players_full = key_players_by_degree_full[:300]\n",
    "\n",
    "top_key_players_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90868493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will now split the data with the corrected labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    key_features_data_scaled, pivot_data_label, test_size=0.2, random_state=42, stratify=pivot_data_label)\n",
    "\n",
    "# Train the logistic regression model using the corrected data\n",
    "logreg = LogisticRegression(max_iter=10000)\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities for the test set\n",
    "y_pred_proba = logreg.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate the AUC-ROC score for the test set\n",
    "auc_roc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "auc_roc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdb1c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Apply SMOTE to the training data only\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Initialize the Random Forest Classifier\n",
    "rf_smote = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Train the Random Forest Classifier on the SMOTE-resampled training data\n",
    "rf_smote.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Predict probabilities for the test set\n",
    "y_pred_proba_rf_smote = rf_smote.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate the AUC-ROC score for the test set using the Random Forest model trained on SMOTE data\n",
    "auc_roc_rf_smote = roc_auc_score(y_test, y_pred_proba_rf_smote)\n",
    "\n",
    "# Output the SMOTE class distribution and the AUC-ROC score\n",
    "smote_class_distribution = pd.Series(y_train_smote).value_counts(), auc_roc_rf_smote\n",
    "\n",
    "auc_roc_rf_smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d8371b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
